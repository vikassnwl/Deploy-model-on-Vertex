{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-26 10:55:58.109333: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-26 10:55:58.113616: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-26 10:55:58.127769: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-26 10:55:58.148135: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-26 10:55:58.154357: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-26 10:55:58.168389: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-26 10:56:00.074926: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\"model/mnist_model_cnn.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_mnist_model/0001/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_mnist_model/0001/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'my_mnist_model/0001'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  140465119516224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140465119521152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140465119692400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140465119697328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"my_mnist_model\"\n",
    "model_version = \"0001\"\n",
    "model_path = Path(model_name) / model_version\n",
    "model.export(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.expand_dims(X_test[:3], axis=-1)/255. # pretend we have 3 new digit images to classify\n",
    "request_json = json.dumps({\n",
    "    \"signature_name\": \"serving_default\",\n",
    "    \"instances\": X_new.tolist(),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"MODEL_DIR\"] = str(model_path.parent.absolute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "tensorflow_model_server \\\n",
    "--port=8500 \\\n",
    "--rest_api_port=8501 \\\n",
    "--model_name=my_mnist_model \\\n",
    "--model_base_path=\"${MODEL_DIR}\" >my_server.log 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using REST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "server_url = \"http://localhost:8501/v1/models/my_mnist_model:predict\"\n",
    "response = requests.post(server_url, data=request_json)\n",
    "response.raise_for_status() # raise an exception in case of error\n",
    "response = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.99, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba = np.array(response[\"predictions\"])\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using gRPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_serving.apis.predict_pb2 import PredictRequest\n",
    "request = PredictRequest()\n",
    "request.model_spec.name = model_name\n",
    "request.model_spec.signature_name = \"serving_default\"\n",
    "input_name = model.inputs[0].name\n",
    "request.inputs[input_name].CopyFrom(tf.make_tensor_proto(X_new, dtype=\"float32\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "channel = grpc.insecure_channel('localhost:8500')\n",
    "predict_service = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "response = predict_service.Predict(request, timeout=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'output_0': dtype: DT_FLOAT\n",
       "tensor_shape {\n",
       "  dim {\n",
       "    size: 3\n",
       "  }\n",
       "  dim {\n",
       "    size: 10\n",
       "  }\n",
       "}\n",
       "float_val: 1.15736533e-07\n",
       "float_val: 4.0681936e-08\n",
       "float_val: 1.35461705e-05\n",
       "float_val: 6.05343157e-05\n",
       "float_val: 1.3926324e-10\n",
       "float_val: 1.60982594e-07\n",
       "float_val: 1.73971879e-11\n",
       "float_val: 0.99991715\n",
       "float_val: 8.18322192e-08\n",
       "float_val: 8.22852508e-06\n",
       "float_val: 1.29933596e-06\n",
       "float_val: 0.000251691235\n",
       "float_val: 0.999696136\n",
       "float_val: 8.13125382e-07\n",
       "float_val: 8.18114586e-12\n",
       "float_val: 2.183703e-08\n",
       "float_val: 1.7922817e-08\n",
       "float_val: 2.05343198e-07\n",
       "float_val: 4.97845394e-05\n",
       "float_val: 1.43922027e-10\n",
       "float_val: 8.15870681e-06\n",
       "float_val: 0.992999911\n",
       "float_val: 7.5164935e-05\n",
       "float_val: 3.6409383e-05\n",
       "float_val: 0.00095823\n",
       "float_val: 6.07134207e-05\n",
       "float_val: 0.000982121\n",
       "float_val: 0.00106801733\n",
       "float_val: 0.00376288965\n",
       "float_val: 4.84777665e-05\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'keras_tensor_9'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.outputs[0].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.outputs[0].name in response.outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_name = model.outputs[0].name\n",
    "# outputs_proto = response.outputs[output_name]\n",
    "outputs_proto = response.outputs[\"output_0\"]\n",
    "y_proba = tf.make_ndarray(outputs_proto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1], dtype=uint8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying a new model version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_mnist_model/0002/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_mnist_model/0002/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'my_mnist_model/0002'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 28, 28, 1), dtype=tf.float32, name='input_layer')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  140280416220992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140280416733488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140280416745808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140280416742464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "# treating previous model as a new version for testing instead of training a new one\n",
    "import copy\n",
    "model2 = copy.deepcopy(model)\n",
    "model_version = \"0002\"\n",
    "model_path = Path(model_name) / model_version\n",
    "model2.export(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At regular intervals (the delay is configurable), TF Serving checks the model directory\n",
    "for new model versions. If it finds one, it automatically handles the transition grace‐\n",
    "fully: by default, it answers pending requests (if any) with the previous model version,\n",
    "while handling new requests with the new version. As soon as every pending request\n",
    "has been answered, the previous model version is unloaded. You can see this at work\n",
    "in the TF Serving logs (in *my_server.log*):\n",
    "```log\n",
    "[...]\n",
    "Reading SavedModel from: /models/my_mnist_model/0002\n",
    "Reading meta graph with tags { serve }\n",
    "[...]\n",
    "Successfully loaded servable version {name: my_mnist_model version: 2}\n",
    "Quiescing servable version {name: my_mnist_model version: 1}\n",
    "Done quiescing servable version {name: my_mnist_model version: 1}\n",
    "Unloading servable version {name: my_mnist_model version: 1}\n",
    "```\n",
    ">If the SavedModel contains some example instances in the assets/\n",
    "extra directory, you can configure TF Serving to run the new\n",
    "model on these instances before starting to use it to serve requests.\n",
    "This is called model warmup: it will ensure that everything is prop‐\n",
    "erly loaded, avoiding long response times for the first requests.\n",
    "\n",
    "This approach offers a smooth transition, but it may use too much RAM—especially\n",
    "GPU RAM, which is generally the most limited. In this case, you can configure TF\n",
    "Serving so that it handles all pending requests with the previous model version and\n",
    "unloads it before loading and using the new model version. This configuration will\n",
    "avoid having two model versions loaded at the same time, but the service will be\n",
    "unavailable for a short period.\n",
    "\n",
    "As you can see, TF Serving makes it straightforward to deploy new models. More‐\n",
    "over, if you discover that version 2 does not work as well as you expected, then rolling\n",
    "back to version 1 is as simple as removing the my_mnist_model/0002 directory.\n",
    "\n",
    ">Another great feature of TF Serving is its automatic batching capa‐\n",
    "bility, which you can activate using the --enable_batching option\n",
    "upon startup. When TF Serving receives multiple requests within\n",
    "a short period of time (the delay is configurable), it will automat‐\n",
    "ically batch them together before using the model. This offers a\n",
    "significant performance boost by leveraging the power of the GPU.\n",
    "Once the model returns the predictions, TF Serving dispatches\n",
    "each prediction to the right client. You can trade a bit of latency\n",
    "for a greater throughput by increasing the batching delay (see the\n",
    "--batching_parameters_file option).\n",
    "\n",
    "If you expect to get many queries per second, you will want to deploy TF Serving\n",
    "on multiple servers and load-balance the queries (see Figure 19-2). This will require\n",
    "deploying and managing many TF Serving containers across these servers. One way\n",
    "to handle that is to use a tool such as Kubernetes, which is an open source system\n",
    "for simplifying container orchestration across many servers. If you do not want to\n",
    "purchase, maintain, and upgrade all the hardware infrastructure, you will want to\n",
    "use virtual machines on a cloud platform such as Amazon AWS, Microsoft Azure,\n",
    "Google Cloud Platform, IBM Cloud, Alibaba Cloud, Oracle Cloud, or some other\n",
    "platform as a service (PaaS) offering. Managing all the virtual machines, handling\n",
    "container orchestration (even with the help of Kubernetes), taking care of TF Serving\n",
    "configuration, tuning and monitoring—all of this can be a full-time job. Fortunately,\n",
    "some service providers can take care of all this for you. In this chapter we will\n",
    "use Vertex AI: it’s the only platform with TPUs today; it supports TensorFlow 2,\n",
    "Scikit-Learn, and XGBoost; and it offers a nice suite of AI services. There are several\n",
    "other providers in this space that are capable of serving TensorFlow models as well,\n",
    "though, such as Amazon AWS SageMaker and Microsoft AI Platform, so make sure\n",
    "to check them out too.\n",
    "\n",
    "![image.png](assets/images/load_balancing.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
